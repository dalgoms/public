# WebScout Agent

**AI-powered website discovery agent for Figma-ready design.**

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Node.js Version](https://img.shields.io/badge/node-%3E%3D18-brightgreen)](https://nodejs.org/)

---

## What is WebScout Agent?

WebScout Agent automatically discovers and maps all URLs from any website, preparing them for Figma design import. Instead of manually collecting URLs one by one, this AI agent:

- **Discovers** URLs via sitemap.xml or intelligent crawling
- **Normalizes** URLs (removes queries, handles www/non-www)
- **Structures** website hierarchy with depth analysis
- **Exports** clean URL lists ready for Figma (html.to.design)

Perfect for design rebuilds, site audits, and Figma workflow automation.

---

## Why I Built This

When rebuilding designs, manually entering URLs one by one is inefficient and error-prone. WebScout Agent automates this entire process, turning hours of manual work into seconds of automated discovery.

---

## Agent Flow

```
Input Domain
    â†“
Sitemap Discovery (robots.txt â†’ sitemap.xml)
    â†“
Crawl Fallback (if no sitemap)
    â†“
URL Normalization (www/non-www merge, query strip)
    â†“
Structure Analysis (depth, path hierarchy)
    â†“
Export (urls.txt / urls.csv)
```

For detailed flow diagram, see [docs/agent-flow.md](./docs/agent-flow.md)

---

## Features

### Core Discovery
- âœ… **Sitemap-first approach** - Automatically detects and parses sitemap.xml
- âœ… **Intelligent fallback** - Crawls internal links if sitemap unavailable
- âœ… **Same-origin filtering** - Only collects URLs from the target domain
- âœ… **Robots.txt parsing** - Discovers sitemap locations automatically

### URL Processing
- âœ… **Query stripping** - Optional removal of query parameters
- âœ… **www/non-www merge** - Treats www and non-www as same site
- âœ… **Hash removal** - Cleans URL fragments automatically
- âœ… **Path exclusion** - Filter out admin, API, or other paths

### Control & Limits
- âœ… **Depth control** - Limit crawling depth (default: 3)
- âœ… **Max URLs limit** - Prevent runaway collection (default: 5000)
- âœ… **Request throttling** - 150ms delay between requests

### Dashboard & Export
- âœ… **Real-time KPI dashboard** - Total URLs, source, depth, excluded count
- âœ… **Interactive URL table** - Search, sort, filter, select
- âœ… **Export formats** - urls.txt (one per line) and urls.csv
- âœ… **Insights tabs** - Sitemap view, page types, Figma readiness, importance

---

## Quick Start

### Prerequisites

- Node.js 18+ ([Download](https://nodejs.org/))
- npm (comes with Node.js)

### Installation

```bash
# Clone the repository
git clone https://github.com/yourusername/webscout-agent.git
cd webscout-agent

# Install dependencies
npm install
```

### Run Web UI

```bash
npm start
```

Open your browser: **http://localhost:3001**

The server automatically finds an available port between 3001-3010.

---

## Using Web UI

1. **Enter Domain** - Type any website URL (e.g., `https://example.com`)
2. **Configure Options** (optional):
   - **Depth**: Maximum crawl depth (default: 3)
   - **Max URLs**: Maximum URLs to collect (default: 5000)
   - **Query Strip**: Remove query parameters
   - **www/non-www**: Merge www and non-www variants
   - **Exclude**: Comma-separated paths to exclude (e.g., `/admin,/api`)
3. **Click "Collect URLs"** - Watch real-time progress
4. **Review Results** - Check KPI cards and URL table
5. **Export** - Download `urls.txt` or `urls.csv`
6. **Import to Figma** - Use [html.to.design](https://html.to.design) to import URLs

### Demo Site

Click **"Use Demo Site"** to test with `https://webscout-demo.pages.dev`

---

## CLI Usage

For headless operation or automation:

```bash
# Basic usage
node collect-urls.mjs https://example.com

# With options
node collect-urls.mjs https://example.com --depth=4 --max=8000

# Output files
# - urls.txt (one URL per line)
# - urls.csv (CSV format)
```

### CLI Options

- `--depth=N` - Maximum crawl depth (default: 3)
- `--max=N` - Maximum URLs to collect (default: 5000)

### Verify Results

On Windows PowerShell:

```powershell
.\verify-urls.ps1 -TargetDomain "https://example.com"
```

This validates:
- âœ… Same-origin filtering
- âœ… Hash removal
- âœ… Query string handling
- âœ… www/non-www normalization

---

## Deployment (Vercel)

### Step 1: Push to GitHub

```bash
git init
git add .
git commit -m "WebScout Agent MVP"
git remote add origin https://github.com/yourusername/webscout-agent.git
git push -u origin main
```

### Step 2: Deploy on Vercel

1. Go to [vercel.com](https://vercel.com)
2. Sign in with GitHub
3. Click **"Add New..."** â†’ **"Project"**
4. Select `webscout-agent` repository
5. Click **"Deploy"**

Vercel automatically detects the configuration from `vercel.json`.

### Environment Variables

None required for basic operation.

---

## Folder Structure

```
webscout-agent/
â”œâ”€â”€ README.md                 # This file
â”œâ”€â”€ CHANGELOG.md              # Version history
â”œâ”€â”€ LICENSE                   # MIT License
â”œâ”€â”€ package.json              # Dependencies & scripts
â”œâ”€â”€ vercel.json               # Vercel deployment config
â”œâ”€â”€ .gitignore                # Git ignore rules
â”‚
â”œâ”€â”€ server.js                 # Express server (Web UI)
â”œâ”€â”€ collect-urls.mjs          # Core URL collection logic
â”œâ”€â”€ verify-urls.ps1          # PowerShell validation script
â”‚
â”œâ”€â”€ public/                   # Web UI frontend
â”‚   â”œâ”€â”€ index.html           # Main HTML
â”‚   â”œâ”€â”€ style.css            # Styles
â”‚   â””â”€â”€ app.js               # Client-side logic
â”‚
â”œâ”€â”€ docs/                     # Documentation
â”‚   â”œâ”€â”€ architecture.md      # System architecture
â”‚   â”œâ”€â”€ agent-flow.md        # Agent flow diagram
â”‚   â””â”€â”€ roadmap.md           # Future features
â”‚
â””â”€â”€ samples/                  # Sample outputs
    â”œâ”€â”€ urls.sample.txt      # Example urls.txt
    â””â”€â”€ urls.sample.csv       # Example urls.csv
```

---

## Architecture

WebScout Agent consists of:

- **Express Server** (`server.js`) - Serves static UI and API endpoints
- **Collection Engine** (`collect-urls.mjs`) - Core URL discovery logic
- **Web Dashboard** (`public/`) - React-free vanilla JS UI
- **Validation Script** (`verify-urls.ps1`) - PowerShell result checker

For detailed architecture, see [docs/architecture.md](./docs/architecture.md)

---

## Screenshots

![WebScout Agent Dashboard](docs/screenshots/dashboard.png)
*Real-time KPI dashboard with insights*

![URL Collection](docs/screenshots/collection.png)
*Interactive URL table with search and export*

---

## Roadmap

### âœ… Completed (v0.1)
- [x] Sitemap discovery
- [x] Internal link crawling
- [x] Web dashboard UI
- [x] URL export (txt/csv)
- [x] KPI metrics
- [x] Insights tabs

### ğŸš§ Planned
- [ ] Sitemap tree visualization
- [ ] Page type auto-tagging (marketing/product/support)
- [ ] Figma readiness scoring
- [ ] Page importance heatmap (link count analysis)
- [ ] Funnel mapping
- [ ] Batch processing
- [ ] API rate limiting
- [ ] Authentication

See [docs/roadmap.md](./docs/roadmap.md) for details.

---

## Troubleshooting

### Port Already in Use

The server automatically tries ports 3001-3010. If all are busy:

```bash
# Kill process on port 3001
# Windows PowerShell
netstat -ano | findstr :3001
taskkill /PID <PID> /F
```

### No URLs Collected

- **Check sitemap**: Visit `https://yourdomain.com/sitemap.xml`
- **SPA sites**: May require JavaScript rendering (not supported yet)
- **Bot blocking**: Some sites block automated requests
- **Try crawl fallback**: Disable sitemap preference

### PowerShell Script Won't Run

```powershell
Set-ExecutionPolicy -Scope Process -ExecutionPolicy Bypass
.\verify-urls.ps1 -TargetDomain "https://example.com"
```

---

## Contributing

Contributions welcome! Please:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

---

## License

MIT License - see [LICENSE](./LICENSE) file for details.

---

## Author

**Seyoung Lee**

Built with â¤ï¸ for designers and developers who hate manual URL collection.

---

## Acknowledgments

- Inspired by the need to automate Figma design workflows
- Built with Express.js, vanilla JavaScript, and modern CSS
- Deployed on Vercel

---

**Ready to scout your website?** Start collecting URLs in seconds! ğŸš€
